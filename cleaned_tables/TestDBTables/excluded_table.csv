excluded_id,excluded
1,"""This produces a very large, but noisy initial set of photographs with associated text. We filter this set of photos so that the descriptions attached to a picture are relevant and visually descriptive.""
"
2,"""We withheld ~2k randomly selected images for testing purposes.""  ""Each image is accompanied by a short caption that describes the content and place of the photo in a free form text. Per our agreement with the photo provider we are not allowed to release these captions.""
"
3,All images for which creators explicitly requested opt-out of AI training.
4,"All samples with less than 5 characters alt-text length or less than 5 KB image size are dropped. All images with the too big resolution, potentially DOS bombs, were dropped before attempting to process them. Duplicate removal is performed with a bloom filter based on URL. Future runs would include more variate deduplication rules, such as URL + language for the multilanguage dataset. We use CLIP respectively MCLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.28 for the English language ( with CLIP B/32) and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected based on human inspection of the test results. We use the CLIP embeddings of images and texts to filter out to the possible extent the illegal content."
5,"Authors apply the following filtering conditions on the WAT files downloaded from Common Crawl: ""All samples with less than 5 character alt-text length or less than 5 KB image size are dropped. Duplicate removal is performed with bloom filter based on URL and alt-text. We use CLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.3. This threshold was selected based on human inspections. We use the CLIP embeddings of images and texts to filter out illegal contents.""
"
6,"Authors report that they have excluded some datasets ""because they were too small to be worth spending time or because the English component of the data did not merit inclusion on its own. Three datasets were excluded for other reasons: (1) US Congressional Records were excluded because it ""reflects the opinions and biases of the political class over the past 200 years, including segregationism and xenophobia."" (2) Online Fanfiction resources amounting to Hundreds of GiB were excluded on logistical grounds. (3) Literotica, platform where users can upload short-form erotic fiction, was excluded because the authors decided to exclude fanfiction, the corpus would require significant investigation, and corpus contain significant amount of stereotyping [[Appendix B]](https://arxiv.org/pdf/2101.00027.pdf).
"
7,"Automated filtering was conducted.
"
8,"Categories such as Relationships and Finance and Business, that may be more abstract, are excluded. Videos with less than 100 views are removed. Authors also ignore videos that have less than 100 words. Videos longer than 2,000 seconds are removed. As some videos may appear in several tasks, the videos are deduplicated based on YouTube IDs."
9,"Code problems easily found on the internet.
"
10,"Data was filtered for English using langdetect. Further, data was filtered to end in terminal punctuation, to remove short pages (less than 5 sentences), and to remove ""Dirty, Naughty, Obscene or Otherwise Bad Words"".
"
11,"Documents that are not in English are excluded.
"
12,"Following were filtered from the dataset: autogenerated files; files with average line length > 100, maximum line length > 1000, or few alphanumeric characters [[Section 3.1]](https://arxiv.org/pdf/2107.03374.pdf#subsection.3.1).
"
13,"GitHub repositories with copyleft licenses were excluded. Programming languageges other than the most common 24 were excluded [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).
"
14,"MS-COCO was excluded from the dataset, but because MS-COCO was created from YFCC100M, some of the test images (not the captions) were included.
"
15,"Removed images less than 5KB image size. Removed images with an aspect ratio greater than 3.0. Removed images with min(width, height) < 200. Removed images with a score of OpenNSFW2 or GantMan/NSFW higher than 0.5. Removed all duplicate images based on the image pHash value from external public datasets. Removed texts with a length of 5 or less. Removed texts that do not have a noun form. Removed texts with less than 3 words or more than 256 words and texts over 1000 in length. Removed texts appearing more than 10 times. Removed texts containing NSFW words. Removed duplicated samples based on (image_phash, text)."
16,"Some of the filtering steps used in the preparation of Conceptual Captions dataset are relaxed to trade off high-recall for low-precision. The following steps are applied in the given order:
1. Image-based Filtering - Only keep JPEG images where both dimensions are greater than 400 pixels, and the ratio of larger to smaller dimension is no more than 2.5. Exclude images that trigger pornography or profanity detectors.
2. Text-based Filtering - Allow text between 3 and 256 words in the alt-text. Discard candidates with no noun or no determiner, but permit ones without prepositions. Set the maximum fraction of word repetition allowed to 0.2. Increase the threshold for counting a word type as rare from 5 to 20.
3. Image&Text-based Filtering - Filter out candidates for which none of the text tokens can be mapped to the content of the image.
"
17,The Common Crawl dataset was processed using a classifier that kept high quality documents and filtered low quality documents. WebText was used as a proxy for high quality documents [[Appendix A]](https://arxiv.org/pdf/2005.14165.pdf#appendix.A).
18,"The LAION-5B dataset is filtered to 2.3B by removing NSFW images using [https://github.com/GantMan/nsfw](https://github.com/GantMan/nsfw), toxic words in text, and images with watermark probability > 0.5. The HD-VILA-100M is randomly subsampled to 10M video clips.
"
19,The data excluded are those that have copyright issues.
20,"The following filtering steps are applied in the given order:
1. Image-based Filtering - ""It only keeps JPEG images where both dimensions are greater than 400 pixels, and the ratio of larger to smaller dimension is no more than 2. It excludes images that trigger pornography or profanity detectors. These filters discard more than 65% of the candidates.""
2. Text-based Filtering - ""Candidates with no determiner, no noun, or no preposition are discarded; candidates with a high noun ratio are also discarded; candidates with a high rate of token repetition are discarded; candidates where the first word is not capitalized, or with too high capitalized-word ratio are discarded; we use a vocabulary VW of 1B token types, appearing at least 5 times in the English Wikipedia, and discard candidates that contain tokens that are not found in this vocabulary. candidates that score too high or too low on the polarity annotations, or trigger the pornography/profanity detectors, are discarded; predefined boiler-plate prefix/suffix sequences matching the text are cropped, e.g. “click to enlarge picture”, “stock photo”; we also drop text which begins/ends in certain patterns, e.g. “embedded image permalink”, “profile photo”. These filters only allow around 3% of the incoming candidates to pass to the later stages.""
3. Image&Text-based Filtering - ""We filter out candidates for which none of the text tokens can be mapped to the content of the image. This filter discards around 60% of the incoming candidates.""
4. Text Transformation with Hypernymization - ""Noun modifiers of certain types (proper nouns, numbers, units) are removed; dates, durations, and preposition-based locations (e.g., ""in Los Angeles"") are removed; named-entities are identified, matched against the KG entries, and substitute with their hypernym; resulting coordination noun-phrases with the same head (e.g., ""actor and actor"") are resolved into a single-head, pluralized form (e.g., ""actors""). Around 20% of samples are discarded during this transformation. We then cluster all resolved entities (e.g., 2560 ""actor"", ""dog"", ""neighborhood"", etc.) and keep only the candidates for which all detected types have a count of over 100 (around 55% of the candidates).""
"
21,"We conservatively decided not to prompt datasets that contain potentially harmful content (for instance, datasets built on social media content)."
22,"We eliminate duplicates, low resolution images, and images potentially contain harmful content from the LAION dataset."
23,YFCC100M is filtered for non-English captions and very short (< 2 word) captions.
24,images with non-derivative licenses
25,unknown
