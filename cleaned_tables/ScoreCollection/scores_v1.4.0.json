[
  {
    "model": "cohere_command-r",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.782
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.551
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.21052631578947367,
            "subset": "abercrombie"
          },
          {
            "score": 0.6857142857142857,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.31607629427792916,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.416,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.9052631578947369,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.3333333333333333,
            "subject": "algebra"
          },
          {
            "score": 0.28205128205128205,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.15789473684210525,
            "subject": "geometry"
          },
          {
            "score": 0.23076923076923078,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.23333333333333334,
            "subject": "number_theory"
          },
          {
            "score": 0.32558139534883723,
            "subject": "prealgebra"
          },
          {
            "score": 0.2982456140350877,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.5546719681908548
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.33,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.45,
            "subject": "college_chemistry"
          },
          {
            "score": 0.78,
            "subject": "computer_security"
          },
          {
            "score": 0.45614035087719296,
            "subject": "econometrics"
          },
          {
            "score": 0.82,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.7417055733551178
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.35223443804297827,
            "mode": "closedbook"
          },
          {
            "score": 0.7204548439351282,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.174791144327399,
            "language_pair": "cs-en"
          },
          {
            "score": 0.16895345594542024,
            "language_pair": "de-en"
          },
          {
            "score": 0.1731811935556508,
            "language_pair": "fr-en"
          },
          {
            "score": 0.10749158055411848,
            "language_pair": "hi-en"
          },
          {
            "score": 0.12190168090921584,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "cohere_command-r-plus",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.828
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.738
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.5368421052631579,
            "subset": "abercrombie"
          },
          {
            "score": 0.8428571428571429,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.42779291553133514,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.603,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.9473684210526315,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.6074074074074074,
            "subject": "algebra"
          },
          {
            "score": 0.38461538461538464,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.3157894736842105,
            "subject": "geometry"
          },
          {
            "score": 0.25,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.36666666666666664,
            "subject": "number_theory"
          },
          {
            "score": 0.5465116279069767,
            "subject": "prealgebra"
          },
          {
            "score": 0.3508771929824561,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.5666003976143141
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.21,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.55,
            "subject": "college_chemistry"
          },
          {
            "score": 0.74,
            "subject": "computer_security"
          },
          {
            "score": 0.5614035087719298,
            "subject": "econometrics"
          },
          {
            "score": 0.89,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.7352347742859234
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.3431863421631031,
            "mode": "closedbook"
          },
          {
            "score": 0.7113734272191162,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.21568392622798432,
            "language_pair": "cs-en"
          },
          {
            "score": 0.18612979533953025,
            "language_pair": "de-en"
          },
          {
            "score": 0.23341855908682252,
            "language_pair": "fr-en"
          },
          {
            "score": 0.1560532870954533,
            "language_pair": "hi-en"
          },
          {
            "score": 0.22522913770310848,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "mistralai_mistral-7b-instruct-v0.3",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.79
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.538
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.3473684210526316,
            "subset": "abercrombie"
          },
          {
            "score": 0.7326530612244898,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.24795640326975477,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.264,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.06315789473684211,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.4222222222222222,
            "subject": "algebra"
          },
          {
            "score": 0.3333333333333333,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.21052631578947367,
            "subject": "geometry"
          },
          {
            "score": 0.11538461538461539,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.23333333333333334,
            "subject": "number_theory"
          },
          {
            "score": 0.47674418604651164,
            "subject": "prealgebra"
          },
          {
            "score": 0.22807017543859648,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.5168986083499006
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.27,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.37,
            "subject": "college_chemistry"
          },
          {
            "score": 0.7,
            "subject": "computer_security"
          },
          {
            "score": 0.42105263157894735,
            "subject": "econometrics"
          },
          {
            "score": 0.79,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.716327555426559
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.2532549871958964,
            "mode": "closedbook"
          },
          {
            "score": 0.6796142755781713,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.15607743543256875,
            "language_pair": "cs-en"
          },
          {
            "score": 0.1541581770170039,
            "language_pair": "de-en"
          },
          {
            "score": 0.17132886172547132,
            "language_pair": "fr-en"
          },
          {
            "score": 0.04704395381301861,
            "language_pair": "hi-en"
          },
          {
            "score": 0.1837031065930425,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "openai_gpt-4-turbo-2024-04-09",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.97
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.824
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.8,
            "subset": "abercrombie"
          },
          {
            "score": 0.8163265306122449,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.41689373297002724,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.656,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.9473684210526315,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.9703703703703703,
            "subject": "algebra"
          },
          {
            "score": 0.9230769230769231,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.7105263157894737,
            "subject": "geometry"
          },
          {
            "score": 0.7884615384615384,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.8,
            "subject": "number_theory"
          },
          {
            "score": 0.9534883720930233,
            "subject": "prealgebra"
          },
          {
            "score": 0.6842105263157895,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.7833001988071571
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.56,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.53,
            "subject": "college_chemistry"
          },
          {
            "score": 0.83,
            "subject": "computer_security"
          },
          {
            "score": 0.6754385964912281,
            "subject": "econometrics"
          },
          {
            "score": 0.96,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.7611681213732712
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.48210297207204034,
            "mode": "closedbook"
          },
          {
            "score": 0.7952101327052921,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.2171163446380068,
            "language_pair": "cs-en"
          },
          {
            "score": 0.2060570885094376,
            "language_pair": "de-en"
          },
          {
            "score": 0.2311874066030324,
            "language_pair": "fr-en"
          },
          {
            "score": 0.16907410423032782,
            "language_pair": "hi-en"
          },
          {
            "score": 0.2642434438757102,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "openai_gpt-4o-2024-05-13",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.966
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.905
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.8421052631578947,
            "subset": "abercrombie"
          },
          {
            "score": 0.7918367346938775,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.44141689373297005,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.601,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.9894736842105263,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.9555555555555556,
            "subject": "algebra"
          },
          {
            "score": 0.9230769230769231,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.631578947368421,
            "subject": "geometry"
          },
          {
            "score": 0.7115384615384616,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.8666666666666667,
            "subject": "number_theory"
          },
          {
            "score": 0.9767441860465116,
            "subject": "prealgebra"
          },
          {
            "score": 0.7368421052631579,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.856858846918489
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.64,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.61,
            "subject": "college_chemistry"
          },
          {
            "score": 0.84,
            "subject": "computer_security"
          },
          {
            "score": 0.7017543859649122,
            "subject": "econometrics"
          },
          {
            "score": 0.95,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.8039471342235485
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.5013491384419004,
            "mode": "closedbook"
          },
          {
            "score": 0.803159876040668,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.2408254375993402,
            "language_pair": "cs-en"
          },
          {
            "score": 0.21616951630215925,
            "language_pair": "de-en"
          },
          {
            "score": 0.24017350786714278,
            "language_pair": "fr-en"
          },
          {
            "score": 0.17573561898945134,
            "language_pair": "hi-en"
          },
          {
            "score": 0.2806031982428227,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "qwen_qwen1.5-110b-chat",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.922
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.815
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.5894736842105263,
            "subset": "abercrombie"
          },
          {
            "score": 0.763265306122449,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.3869209809264305,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.423,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.9578947368421052,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.7555555555555555,
            "subject": "algebra"
          },
          {
            "score": 0.7692307692307693,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.21052631578947367,
            "subject": "geometry"
          },
          {
            "score": 0.28846153846153844,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.5333333333333333,
            "subject": "number_theory"
          },
          {
            "score": 0.7674418604651163,
            "subject": "prealgebra"
          },
          {
            "score": 0.6491228070175439,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.6401590457256461
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.57,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.62,
            "subject": "college_chemistry"
          },
          {
            "score": 0.82,
            "subject": "computer_security"
          },
          {
            "score": 0.6403508771929824,
            "subject": "econometrics"
          },
          {
            "score": 0.87,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.7207089322288743
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.3498619896871144,
            "mode": "closedbook"
          },
          {
            "score": 0.739454568054656,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.2043751433009127,
            "language_pair": "cs-en"
          },
          {
            "score": 0.1863679468776228,
            "language_pair": "de-en"
          },
          {
            "score": 0.2062975928648453,
            "language_pair": "fr-en"
          },
          {
            "score": 0.1330869134486783,
            "language_pair": "hi-en"
          },
          {
            "score": 0.2317609892093071,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "qwen_qwen2-72b-instruct",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.954
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.92
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.7263157894736842,
            "subset": "abercrombie"
          },
          {
            "score": 0.8204081632653061,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.4114441416893733,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.653,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.9473684210526315,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.8888888888888888,
            "subject": "algebra"
          },
          {
            "score": 0.7948717948717948,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.6052631578947368,
            "subject": "geometry"
          },
          {
            "score": 0.6730769230769231,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.8333333333333334,
            "subject": "number_theory"
          },
          {
            "score": 0.9302325581395349,
            "subject": "prealgebra"
          },
          {
            "score": 0.8070175438596491,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.7455268389662028
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.67,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.65,
            "subject": "college_chemistry"
          },
          {
            "score": 0.85,
            "subject": "computer_security"
          },
          {
            "score": 0.7368421052631579,
            "subject": "econometrics"
          },
          {
            "score": 0.94,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.7271789738821858
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.38969418493921437,
            "mode": "closedbook"
          },
          {
            "score": 0.7758053578298147,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.20916018055359972,
            "language_pair": "cs-en"
          },
          {
            "score": 0.19464934426945604,
            "language_pair": "de-en"
          },
          {
            "score": 0.2183575090300165,
            "language_pair": "fr-en"
          },
          {
            "score": 0.15564765250538187,
            "language_pair": "hi-en"
          },
          {
            "score": 0.25536515402337867,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  },
  {
    "model": "snowflake_snowflake-arctic-instruct",
    "tests": [
      {
        "test_name": "commonsense",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.828
          }
        ]
      },
      {
        "test_name": "gsm",
        "metric": "final_number_exact_match",
        "subsets": [
          {
            "score": 0.768
          }
        ]
      },
      {
        "test_name": "legalbench",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.42105263157894735,
            "subset": "abercrombie"
          },
          {
            "score": 0.7816326530612245,
            "subset": "corporate_lobbying"
          },
          {
            "score": 0.35149863760217986,
            "subset": "function_of_decision_section"
          },
          {
            "score": 0.512,
            "subset": "international_citizenship_questions"
          },
          {
            "score": 0.8736842105263158,
            "subset": "proa"
          }
        ]
      },
      {
        "test_name": "math",
        "metric": "math_equiv_chain_of_thought",
        "subsets": [
          {
            "score": 0.7851851851851852,
            "subject": "algebra"
          },
          {
            "score": 0.5641025641025641,
            "subject": "counting_and_probability"
          },
          {
            "score": 0.3157894736842105,
            "subject": "geometry"
          },
          {
            "score": 0.3269230769230769,
            "subject": "intermediate_algebra"
          },
          {
            "score": 0.4666666666666667,
            "subject": "number_theory"
          },
          {
            "score": 0.7209302325581395,
            "subject": "prealgebra"
          },
          {
            "score": 0.45614035087719296,
            "subject": "precalculus"
          }
        ]
      },
      {
        "test_name": "med_qa",
        "metric": "quasi_exact_match",
        "subsets": [
          {
            "score": 0.5805168986083499
          }
        ]
      },
      {
        "test_name": "mmlu",
        "metric": "exact_match",
        "subsets": [
          {
            "score": 0.31,
            "subject": "abstract_algebra"
          },
          {
            "score": 0.43,
            "subject": "college_chemistry"
          },
          {
            "score": 0.8,
            "subject": "computer_security"
          },
          {
            "score": 0.45614035087719296,
            "subject": "econometrics"
          },
          {
            "score": 0.88,
            "subject": "us_foreign_policy"
          }
        ]
      },
      {
        "test_name": "narrative_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.6535175183610216
          }
        ]
      },
      {
        "test_name": "natural_qa",
        "metric": "f1_score",
        "subsets": [
          {
            "score": 0.3897564598181023,
            "mode": "closedbook"
          },
          {
            "score": 0.5864529300238688,
            "mode": "openbook_longans"
          }
        ]
      },
      {
        "test_name": "wmt_14",
        "metric": "bleu_4",
        "subsets": [
          {
            "score": 0.18315700949340089,
            "language_pair": "cs-en"
          },
          {
            "score": 0.16625966131852649,
            "language_pair": "de-en"
          },
          {
            "score": 0.20441657197096322,
            "language_pair": "fr-en"
          },
          {
            "score": 0.08971771871732837,
            "language_pair": "hi-en"
          },
          {
            "score": 0.2168462391681858,
            "language_pair": "ru-en"
          }
        ]
      }
    ]
  }
]