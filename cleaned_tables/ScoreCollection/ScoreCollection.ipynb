{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f50614-c6e4-404e-bf1e-f664aeb001e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60bf8c-30da-4ee0-811e-2c0e2fc7ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the JSON file\n",
    "file_path = Path('benchmark_output_lite/runs/v1.5.0')\n",
    "test_path = ''\n",
    "model_path = r'commonsense,dataset=openbookqa,method=multiple_choice_joint,model=google_gemini-1.5-flash-001/stats.json'\n",
    "\n",
    "path = Path('benchmark_output_lite/runs/v1.5.0/commonsense,dataset=openbookqa,method=multiple_choice_joint,model=google_gemini-1.5-flash-001/stats.json')\n",
    "# Concatenate paths\n",
    "full_path = file_path / model_path\n",
    "\n",
    "# Print the constructed path to verify\n",
    "print(f\"Constructed path: {full_path}\")\n",
    "\n",
    "# Ensure the constructed path exists before trying to open it\n",
    "if path.exists():\n",
    "    # Open and load the JSON file\n",
    "    with path.open('r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Accessing the data\n",
    "    pprint(data)\n",
    "else:\n",
    "    print(f\"The file at {full_path} does not exist.\")\n",
    "\n",
    "\n",
    "# quasi_exact_match = score\n",
    "# exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9640db20-146d-4b28-96dd-71af6f1a5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['commonsense', 'dataset=openbookqa', 'method=multiple_choice_joint', 'model=01-ai_yi-large-preview\\\\stats.json']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = Path('benchmark_output_lite/runs/v1.5.0')\n",
    "\n",
    "# Initialize a dictionary to store the data\n",
    "results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "# Loop through all directories\n",
    "for json_file in root_dir.rglob('stats.json'):\n",
    "    \n",
    "    # Extract information from the directory name\n",
    "    parts = str(json_file.relative_to(root_dir)).split(',')\n",
    "    if len(parts) >= 4:\n",
    "        test_name = parts[0]\n",
    "        dataset = parts[1].split('=')[1]\n",
    "        method = parts[2].split('=')[1]\n",
    "        model_name = parts[3].split('=')[1].split('-')[1]\n",
    "    print(parts)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3ff43e-7451-4c6e-bc3c-ff6a2a124107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"model\": \"01-ai_yi-large-preview\",\n",
      "    \"tests\": [\n",
      "      {\n",
      "        \"test_name\": \"commonsense\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.946\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"gsm\",\n",
      "        \"metric\": \"final_number_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.69\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"legalbench\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.7368421052631579,\n",
      "            \"subset\": \"abercrombie\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.14489795918367346,\n",
      "            \"subset\": \"corporate_lobbying\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.23705722070844687,\n",
      "            \"subset\": \"function_of_decision_section\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.591,\n",
      "            \"subset\": \"international_citizenship_questions\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8842105263157894,\n",
      "            \"subset\": \"proa\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"math\",\n",
      "        \"metric\": \"math_equiv_chain_of_thought\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.8740740740740741,\n",
      "            \"subject\": \"algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7692307692307693,\n",
      "            \"subject\": \"counting_and_probability\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.5526315789473685,\n",
      "            \"subject\": \"geometry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.5576923076923077,\n",
      "            \"subject\": \"intermediate_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8,\n",
      "            \"subject\": \"number_theory\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7790697674418605,\n",
      "            \"subject\": \"prealgebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6491228070175439,\n",
      "            \"subject\": \"precalculus\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"med_qa\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.6600397614314115\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"mmlu\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.6,\n",
      "            \"subject\": \"abstract_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.52,\n",
      "            \"subject\": \"college_chemistry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.86,\n",
      "            \"subject\": \"computer_security\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7280701754385965,\n",
      "            \"subject\": \"econometrics\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.85,\n",
      "            \"subject\": \"us_foreign_policy\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"narrative_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.37282202834298434\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"natural_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.42769608198028625,\n",
      "            \"mode\": \"closedbook\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.5864410195534986,\n",
      "            \"mode\": \"openbook_longans\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"wmt_14\",\n",
      "        \"metric\": \"bleu_4\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.21171286504538103,\n",
      "            \"language_pair\": \"cs-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.17540612783606496,\n",
      "            \"language_pair\": \"de-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.21829904451109755,\n",
      "            \"language_pair\": \"fr-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.14923454786363446,\n",
      "            \"language_pair\": \"hi-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.1256687442986824,\n",
      "            \"language_pair\": \"ru-en\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"model\": \"anthropic_claude-3-5-sonnet-20240620\",\n",
      "    \"tests\": [\n",
      "      {\n",
      "        \"test_name\": \"commonsense\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.972\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"gsm\",\n",
      "        \"metric\": \"final_number_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.949\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"legalbench\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.8,\n",
      "            \"subset\": \"abercrombie\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7346938775510204,\n",
      "            \"subset\": \"corporate_lobbying\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.4550408719346049,\n",
      "            \"subset\": \"function_of_decision_section\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.575,\n",
      "            \"subset\": \"international_citizenship_questions\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.968421052631579,\n",
      "            \"subset\": \"proa\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"math\",\n",
      "        \"metric\": \"math_equiv_chain_of_thought\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.8222222222222222,\n",
      "            \"subject\": \"algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8205128205128205,\n",
      "            \"subject\": \"counting_and_probability\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.5789473684210527,\n",
      "            \"subject\": \"geometry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8269230769230769,\n",
      "            \"subject\": \"intermediate_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.9333333333333333,\n",
      "            \"subject\": \"number_theory\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.9534883720930233,\n",
      "            \"subject\": \"prealgebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7543859649122807,\n",
      "            \"subject\": \"precalculus\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"med_qa\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.8250497017892644\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"mmlu\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.75,\n",
      "            \"subject\": \"abstract_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.59,\n",
      "            \"subject\": \"college_chemistry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.89,\n",
      "            \"subject\": \"computer_security\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8070175438596491,\n",
      "            \"subject\": \"econometrics\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.96,\n",
      "            \"subject\": \"us_foreign_policy\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"narrative_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.7462731245648014\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"natural_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.5015987604513761,\n",
      "            \"mode\": \"closedbook\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7493178538118532,\n",
      "            \"mode\": \"openbook_longans\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"wmt_14\",\n",
      "        \"metric\": \"bleu_4\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.23034318702908446,\n",
      "            \"language_pair\": \"cs-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.22782444725576256,\n",
      "            \"language_pair\": \"de-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.23525202257259342,\n",
      "            \"language_pair\": \"fr-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.18138759205222438,\n",
      "            \"language_pair\": \"hi-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.2701113668319304,\n",
      "            \"language_pair\": \"ru-en\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"model\": \"google_gemini-1.0-pro-002\",\n",
      "    \"tests\": [\n",
      "      {\n",
      "        \"test_name\": \"commonsense\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.788\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"gsm\",\n",
      "        \"metric\": \"final_number_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.816\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"legalbench\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.5894736842105263,\n",
      "            \"subset\": \"abercrombie\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.11836734693877551,\n",
      "            \"subset\": \"corporate_lobbying\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.4032697547683924,\n",
      "            \"subset\": \"function_of_decision_section\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.454,\n",
      "            \"subset\": \"international_citizenship_questions\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8105263157894737,\n",
      "            \"subset\": \"proa\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"math\",\n",
      "        \"metric\": \"math_equiv_chain_of_thought\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.8592592592592593,\n",
      "            \"subject\": \"algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6410256410256411,\n",
      "            \"subject\": \"counting_and_probability\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.5526315789473685,\n",
      "            \"subject\": \"geometry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.5961538461538461,\n",
      "            \"subject\": \"intermediate_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6,\n",
      "            \"subject\": \"number_theory\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7558139534883721,\n",
      "            \"subject\": \"prealgebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6491228070175439,\n",
      "            \"subject\": \"precalculus\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"med_qa\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.4831013916500994\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"mmlu\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.27,\n",
      "            \"subject\": \"abstract_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.42,\n",
      "            \"subject\": \"college_chemistry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.69,\n",
      "            \"subject\": \"computer_security\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.4824561403508772,\n",
      "            \"subject\": \"econometrics\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.81,\n",
      "            \"subject\": \"us_foreign_policy\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"narrative_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.7506089527885245\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"natural_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.3905657523185001,\n",
      "            \"mode\": \"closedbook\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7141677669834366,\n",
      "            \"mode\": \"openbook_longans\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"wmt_14\",\n",
      "        \"metric\": \"bleu_4\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.2311621341929641,\n",
      "            \"language_pair\": \"cs-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.20540570815247217,\n",
      "            \"language_pair\": \"de-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.21795342237567317,\n",
      "            \"language_pair\": \"fr-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.14404041650455712,\n",
      "            \"language_pair\": \"hi-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.1736110401522043,\n",
      "            \"language_pair\": \"ru-en\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"model\": \"google_gemini-1.5-flash-001\",\n",
      "    \"tests\": [\n",
      "      {\n",
      "        \"test_name\": \"commonsense\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.928\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"gsm\",\n",
      "        \"metric\": \"final_number_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.785\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"legalbench\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.5789473684210527,\n",
      "            \"subset\": \"abercrombie\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.789795918367347,\n",
      "            \"subset\": \"corporate_lobbying\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.4250681198910082,\n",
      "            \"subset\": \"function_of_decision_section\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.543,\n",
      "            \"subset\": \"international_citizenship_questions\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.968421052631579,\n",
      "            \"subset\": \"proa\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"math\",\n",
      "        \"metric\": \"math_equiv_chain_of_thought\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.8888888888888888,\n",
      "            \"subject\": \"algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7692307692307693,\n",
      "            \"subject\": \"counting_and_probability\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.631578947368421,\n",
      "            \"subject\": \"geometry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6538461538461539,\n",
      "            \"subject\": \"intermediate_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7333333333333333,\n",
      "            \"subject\": \"number_theory\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8023255813953488,\n",
      "            \"subject\": \"prealgebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7894736842105263,\n",
      "            \"subject\": \"precalculus\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"med_qa\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.679920477137177\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"mmlu\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.58,\n",
      "            \"subject\": \"abstract_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6,\n",
      "            \"subject\": \"college_chemistry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.79,\n",
      "            \"subject\": \"computer_security\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6140350877192983,\n",
      "            \"subject\": \"econometrics\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.93,\n",
      "            \"subject\": \"us_foreign_policy\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"narrative_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.7828677496382116\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"natural_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.33241428445406646,\n",
      "            \"mode\": \"closedbook\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7232397428872679,\n",
      "            \"mode\": \"openbook_longans\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"wmt_14\",\n",
      "        \"metric\": \"bleu_4\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.23155878999827426,\n",
      "            \"language_pair\": \"cs-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.21372880556145774,\n",
      "            \"language_pair\": \"de-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.2406581055724361,\n",
      "            \"language_pair\": \"fr-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.18608984738647275,\n",
      "            \"language_pair\": \"hi-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.25347957580346714,\n",
      "            \"language_pair\": \"ru-en\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"model\": \"google_gemini-1.5-pro-001\",\n",
      "    \"tests\": [\n",
      "      {\n",
      "        \"test_name\": \"commonsense\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.902\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"gsm\",\n",
      "        \"metric\": \"final_number_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.836\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"legalbench\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.7894736842105263,\n",
      "            \"subset\": \"abercrombie\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8612244897959184,\n",
      "            \"subset\": \"corporate_lobbying\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.4604904632152589,\n",
      "            \"subset\": \"function_of_decision_section\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.675,\n",
      "            \"subset\": \"international_citizenship_questions\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 1.0,\n",
      "            \"subset\": \"proa\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"math\",\n",
      "        \"metric\": \"math_equiv_chain_of_thought\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.9555555555555556,\n",
      "            \"subject\": \"algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8717948717948718,\n",
      "            \"subject\": \"counting_and_probability\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7105263157894737,\n",
      "            \"subject\": \"geometry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.6923076923076923,\n",
      "            \"subject\": \"intermediate_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.8,\n",
      "            \"subject\": \"number_theory\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.9534883720930233,\n",
      "            \"subject\": \"prealgebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7894736842105263,\n",
      "            \"subject\": \"precalculus\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"med_qa\",\n",
      "        \"metric\": \"quasi_exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.6918489065606361\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"mmlu\",\n",
      "        \"metric\": \"exact_match\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.75,\n",
      "            \"subject\": \"abstract_algebra\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.62,\n",
      "            \"subject\": \"college_chemistry\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.83,\n",
      "            \"subject\": \"computer_security\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7280701754385965,\n",
      "            \"subject\": \"econometrics\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.93,\n",
      "            \"subject\": \"us_foreign_policy\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"narrative_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.7825409539853793\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"natural_qa\",\n",
      "        \"metric\": \"f1_score\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.37802870840922326,\n",
      "            \"mode\": \"closedbook\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.7481250574384422,\n",
      "            \"mode\": \"openbook_longans\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"test_name\": \"wmt_14\",\n",
      "        \"metric\": \"bleu_4\",\n",
      "        \"subsets\": [\n",
      "          {\n",
      "            \"score\": 0.24556013264656876,\n",
      "            \"language_pair\": \"cs-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.18709913251551805,\n",
      "            \"language_pair\": \"de-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.25153855943082887,\n",
      "            \"language_pair\": \"fr-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.11781184200882963,\n",
      "            \"language_pair\": \"hi-en\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.1444894297174213,\n",
      "            \"language_pair\": \"ru-en\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the root directory\n",
    "root_dir = Path('benchmark_output_lite/runs/v1.5.0')\n",
    "\n",
    "\n",
    "# Define the specific metrics for each test\n",
    "test_metrics = {\n",
    "    'narrative_qa': 'f1_score',\n",
    "    'natural_qa': 'f1_score',\n",
    "    'commonsense': 'exact_match',\n",
    "    'openbook': 'exact_match',\n",
    "    'mmlu': 'exact_match',\n",
    "    'wmt_14': 'bleu_4',\n",
    "    'med_qa': 'quasi_exact_match',\n",
    "    'math': 'math_equiv_chain_of_thought',\n",
    "    'legalbench': 'quasi_exact_match',\n",
    "    'gsm': 'final_number_exact_match'\n",
    "}\n",
    "\n",
    "# Function to extract information and read stats.json\n",
    "def process_json_file(json_file):\n",
    "    \n",
    "    # Extract information from the directory name\n",
    "    parts = str(json_file.relative_to(root_dir)).split(',')\n",
    "    test_name = parts[0]\n",
    "    dataset = method = model_name = subset = subject = mode = lang_pair = None\n",
    "    if test_name == 'commonsense': \n",
    "        dataset = parts[1].split('=')[1]\n",
    "        method = parts[2].split('=')[1]\n",
    "        model_name = parts[3].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'gsm':       \n",
    "        model_name = parts[1].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'legalbench':\n",
    "        subset = parts[1].split('=')[1]\n",
    "        model_name = parts[2].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'math':\n",
    "        subject = parts[1].split('=')[1]      \n",
    "        model_name = parts[5].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'med_qa':\n",
    "        model_name = parts[1].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'mmlu':\n",
    "        subject = parts[1].split('=')[1]      \n",
    "        method = parts[2].split('=')[1]\n",
    "        model_name = parts[3].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'narrative_qa':\n",
    "        model_name = parts[1].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'natural_qa':\n",
    "        mode = parts[1].split('=')[1]      \n",
    "        model_name = parts[2].split('=')[1].split(os.sep)[0]\n",
    "    elif test_name == 'wmt_14':\n",
    "        lang_pair = parts[1].split('=')[1]\n",
    "        model_name = parts[2].split('=')[1].split(os.sep)[0]\n",
    "    else:\n",
    "        print(test_name + ' isn\\'t set up')\n",
    "\n",
    "    # Read the stats.json file\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "\n",
    "     # Extract the score using the specific metric for the test\n",
    "    metric = test_metrics.get(test_name, None)\n",
    "    split = 'valid' if test_name == 'natural_qa' else 'test'\n",
    "    if not metric:\n",
    "        print(f\"No metric defined for test {test_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract the score\n",
    "    score = None\n",
    "    for entry in data:\n",
    "        if entry['name']['name'] == metric and entry['name']['split'] == split:\n",
    "            score = entry['mean']\n",
    "            #print(f'score: {score}')\n",
    "            break\n",
    "   \n",
    "    if score is not None:\n",
    "       \n",
    "         # Structure the extracted data\n",
    "        test_result = {\n",
    "            #'model_name': model_name,\n",
    "            'test_name': test_name,\n",
    "            'metric': metric,\n",
    "            'score': score\n",
    "        }\n",
    "        \n",
    "        if dataset:\n",
    "            test_result['dataset'] = dataset\n",
    "        if method:\n",
    "            test_result['method'] = method\n",
    "        if subset:\n",
    "            test_result['subset'] = subset\n",
    "        if subject:\n",
    "            test_result['subject'] = subject\n",
    "        if mode:\n",
    "            test_result['mode'] = mode\n",
    "        if lang_pair:\n",
    "            test_result['language_pair'] = lang_pair\n",
    "        \n",
    "        return model_name, test_result\n",
    "    else:\n",
    "        print(f\"No exact match test score found in {json_file}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Loop through all directories and process stats.json files\n",
    "for json_file in root_dir.rglob('stats.json'):\n",
    "    result = process_json_file(json_file)\n",
    "    if result:\n",
    "        model_name, test_result = result\n",
    "        test_name = test_result['test_name']\n",
    "        \n",
    "         # Check for subset or subject and organize accordingly\n",
    "        subset_info = {\n",
    "            'score': test_result['score']\n",
    "        }\n",
    "        if 'subset' in test_result:\n",
    "            subset_info['subset'] = test_result['subset']\n",
    "        if 'subject' in test_result:\n",
    "            subset_info['subject'] = test_result['subject']\n",
    "        if 'mode' in test_result:\n",
    "            subset_info['mode'] = test_result['mode']\n",
    "        if 'language_pair' in test_result:\n",
    "            subset_info['language_pair'] = test_result['language_pair']\n",
    "        \n",
    "        model_results[model_name][test_name].append(subset_info)\n",
    "\n",
    "\n",
    "# Convert to desired JSON format\n",
    "final_results = []\n",
    "for model_name, tests in model_results.items():\n",
    "    model_entry = {\n",
    "        'model': model_name,\n",
    "        'tests': []\n",
    "    }\n",
    "    for test_name, results in tests.items():\n",
    "        test_entry = {\n",
    "            'test_name': test_name,\n",
    "            'metric': test_metrics[test_name],\n",
    "            'subsets': results\n",
    "        }\n",
    "        model_entry['tests'].append(test_entry)\n",
    "\n",
    "    final_results.append(model_entry)\n",
    "    \n",
    "# Print the final JSON document\n",
    "json_object = json.dumps(final_results, indent=2)\n",
    "print(json_object)\n",
    "# Writing to sample.json\n",
    "with open(\"scores_v1.5.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
